#!/usr/bin/env python3
"""
Qwen3-Reranker FastAPI æœåŠ¡
åŸºäºå®˜æ–¹æ¨èçš„ transformers æ–¹å¼å®ç°
æ”¯æŒ OpenAI å…¼å®¹çš„ /rerank æ¥å£
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Qwen3-Reranker Service",
    description="Text reranking service using Qwen3-Reranker-0.6B",
    version="1.0.0"
)

# å…¨å±€å˜é‡
model = None
tokenizer = None
token_true_id = None
token_false_id = None
prefix_tokens = None
suffix_tokens = None
MAX_LENGTH = 8192

class RerankRequest(BaseModel):
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    documents: Optional[List[str]] = Field(None, description="å¾…æ’åºçš„æ–‡æ¡£åˆ—è¡¨")
    texts: Optional[List[str]] = Field(None, description="å¾…æ’åºçš„æ–‡æœ¬åˆ—è¡¨ï¼ˆä¸documentsç­‰æ•ˆï¼‰")
    top_n: Optional[int] = Field(None, description="è¿”å›å‰Nä¸ªç»“æœ")
    return_documents: bool = Field(True, description="æ˜¯å¦è¿”å›æ–‡æ¡£å†…å®¹")
    instruction: Optional[str] = Field(
        None, 
        description="è‡ªå®šä¹‰æŒ‡ä»¤ï¼ˆé»˜è®¤ä¸ºé€šç”¨æ£€ç´¢æŒ‡ä»¤ï¼‰"
    )

class RerankResult(BaseModel):
    index: int = Field(..., description="åŸå§‹æ–‡æ¡£åœ¨è¾“å…¥åˆ—è¡¨ä¸­çš„ç´¢å¼•")
    relevance_score: float = Field(..., description="ç›¸å…³æ€§åˆ†æ•°ï¼ŒèŒƒå›´ 0-1")

class RerankResponse(BaseModel):
    id: str = Field(default="", description="è¯·æ±‚ ID")
    results: List[RerankResult] = Field(..., description="é‡æ’åºç»“æœåˆ—è¡¨")
    meta: Dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®ä¿¡æ¯")

def format_instruction(instruction: str, query: str, doc: str) -> str:
    """æ ¼å¼åŒ–è¾“å…¥"""
    if instruction is None:
        instruction = 'Given a web search query, retrieve relevant passages that answer the query'
    return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"

def process_inputs(pairs: List[str]):
    """é¢„å¤„ç†è¾“å…¥"""
    inputs = tokenizer(
        pairs, 
        padding=False, 
        truncation='longest_first',
        return_attention_mask=False, 
        max_length=MAX_LENGTH - len(prefix_tokens) - len(suffix_tokens)
    )
    
    for i, ele in enumerate(inputs['input_ids']):
        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens
    
    inputs = tokenizer.pad(inputs, padding=True, return_tensors="pt", max_length=MAX_LENGTH)
    
    # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    for key in inputs:
        inputs[key] = inputs[key].to(model.device)
    
    return inputs

@torch.no_grad()
def compute_scores(inputs) -> List[float]:
    """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
    batch_scores = model(**inputs).logits[:, -1, :]
    true_vector = batch_scores[:, token_true_id]
    false_vector = batch_scores[:, token_false_id]
    batch_scores = torch.stack([false_vector, true_vector], dim=1)
    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
    scores = batch_scores[:, 1].exp().tolist()
    return scores

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens
    
    model_path = os.getenv("MODEL_PATH", "Qwen/Qwen3-Reranker-0.6B")
    logger.info(f"Loading model from: {model_path}")
    
    try:
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')
        logger.info("âœ“ Tokenizer loaded")
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if torch.cuda.is_available():
            device = "cuda"
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
            logger.info(f"ğŸ® GPU detected: {gpu_name} (count: {gpu_count})")
            logger.info(f"Using device: {device}")
            
            # GPU æ¨¡å¼ï¼šä½¿ç”¨ float16 å’Œ device_map
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            ).eval()
        else:
            device = "cpu"
            logger.info("ğŸ’» No GPU detected, using CPU")
            logger.info(f"Using device: {device}")
            
            # CPU æ¨¡å¼ï¼šä½¿ç”¨ float32
            model = AutoModelForCausalLM.from_pretrained(model_path).eval()
        
        logger.info(f"âœ“ Model loaded on {device}")
        logger.info(f"  Model type: {model.__class__.__name__}")
        logger.info(f"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
        
        # å‡†å¤‡ç‰¹æ®Š tokens
        token_false_id = tokenizer.convert_tokens_to_ids("no")
        token_true_id = tokenizer.convert_tokens_to_ids("yes")
        
        # å‡†å¤‡ prefix å’Œ suffix
        prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)
        suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)
        
        logger.info("âœ“ Service ready")
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "name": "Qwen3-Reranker Service",
        "model": "Qwen/Qwen3-Reranker-0.6B",
        "version": "1.0.0",
        "endpoints": ["/health", "/rerank", "/v1/rerank"]
    }

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "model": "Qwen/Qwen3-Reranker-0.6B",
        "device": str(model.device) if model else "not loaded"
    }

@app.post("/rerank", response_model=RerankResponse)
async def rerank(request: RerankRequest):
    """é‡æ’åºæ¥å£"""
    try:
        # è·å–æ–‡æ¡£åˆ—è¡¨
        documents = request.documents or request.texts
        if not documents:
            raise HTTPException(
                status_code=400, 
                detail="Either 'documents' or 'texts' field is required"
            )
        
        if len(documents) == 0:
            raise HTTPException(status_code=400, detail="Documents list cannot be empty")
        
        # æ ¼å¼åŒ–è¾“å…¥
        instruction = request.instruction
        pairs = [
            format_instruction(instruction, request.query, doc) 
            for doc in documents
        ]
        
        # å¤„ç†è¾“å…¥
        inputs = process_inputs(pairs)
        
        # è®¡ç®—åˆ†æ•°
        scores = compute_scores(inputs)
        
        # æ„å»ºç»“æœ
        results = [
            RerankResult(
                index=i,
                relevance_score=score
            )
            for i, score in enumerate(scores)
        ]
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # é™åˆ¶è¿”å›æ•°é‡
        if request.top_n:
            results = results[:request.top_n]
        
        # ç”Ÿæˆè¯·æ±‚ ID
        import uuid
        request_id = f"rerank-{uuid.uuid4().hex[:16]}"
        
        return RerankResponse(
            id=request_id,
            results=results,
            meta={
                "api_version": {"version": "1"},
                "billed_units": {"search_units": len(documents)}
            }
        )
        
    except Exception as e:
        logger.error(f"Rerank error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/rerank", response_model=RerankResponse)
async def v1_rerank(request: RerankRequest):
    """OpenAI å…¼å®¹çš„é‡æ’åºæ¥å£"""
    return await rerank(request)

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")
